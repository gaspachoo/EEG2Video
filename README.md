# EEG2Video

This project aims to convert EEG (electroencephalographic) signals into video sequences using deep learning models, including Transformer-based models, 3D UNets, and diffusion pipelines.

## ğŸ“ Project Structure

```
EEG2Video/
â”‚
â”œâ”€â”€ EEG-VP/                         # EEG classification module
â”‚   â”œâ”€â”€ EEG_VP_train_test.py       # EEG model training and testing
â”‚   â””â”€â”€ models.py                  # Neural network architectures for EEG
â”‚
â”œâ”€â”€ EEG2Video/                     # Main EEG-to-Video model components
â”‚   â”œâ”€â”€ 40_class_run_metrics.py   # Evaluation metrics (MSE, SSIM, CLIP, etc.)
â”‚   â”œâ”€â”€ inference_eeg2video.py    # Inference script to generate video from EEG
â”‚   â”œâ”€â”€ train_finetune_videodiffusion.py # Pipeline fine-tuning entry point
â”‚   â”œâ”€â”€ models/                   # Models for video generation
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ resnet.py
â”‚   â”‚   â”œâ”€â”€ train_semantic_predictor.py
â”‚   â”‚   â”œâ”€â”€ unet.py
â”‚   â”‚   â””â”€â”€ unet_blocks.py
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ pipeline_tuneavideo.py
â”‚       â””â”€â”€ pipeline_tuneeeg2video.py
â”‚
â”œâ”€â”€ EEG_preprocessing/            # EEG feature extraction scripts
â”‚   â”œâ”€â”€ DE_PSD.py
â”‚   â”œâ”€â”€ extract_DE_PSD_features_1per1s.py
â”‚   â”œâ”€â”€ extract_DE_PSD_features_1per2s.py
â”‚   â””â”€â”€ segment_raw_signals_200Hz.py
â”‚
â””â”€â”€ project/                      # Data manipulation utilities
    â”œâ”€â”€ import.py
    â””â”€â”€ segment_data.py
```

## ğŸ” Key Components

### EEG-VP
- `EEG_VP_train_test.py`: Data loading, accuracy metrics, training for EEG models.
- `models.py`: Contains architectures like `shallownet`, `eegnet`, `conformer`, `glfnet`, `mlpnet`, etc.

### EEG2Video
- `train_finetune_videodiffusion.py`: Training entry point for the video generation pipeline.
- `inference_eeg2video.py`: Generates video sequences from EEG inputs.
- `models/`:
  - `attention.py`: 3D transformer blocks.
  - `unet.py` & `unet_blocks.py`: 3D conditional UNet with cross-attention.
  - `train_semantic_predictor.py`: CLIP-based semantic predictor module.
- `pipelines/`:
  - `pipeline_tuneavideo.py`: Video generation pipeline.
  - `pipeline_tuneeeg2video.py`: EEG-adapted video generation pipeline.

### EEG_preprocessing
- `DE_PSD.py`: DE/PSD feature extraction.
- `extract_DE_PSD_features_1per1s.py`, `1per2s.py`: Feature extraction over time windows.
- `segment_raw_signals_200Hz.py`: Segments raw EEG signals into time windows.

### project
- `import.py`: Load and visualize EEG blocks.
- `segment_data.py`: Custom segmentation of EEG data for training.

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/EEG2Video.git
   cd EEG2Video
   ```

2. Set up a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # or .venv\Scripts\activate on Windows
   pip install -r requirements.txt
   ```

3. Run training or inference:
   ```bash
   python EEG2Video/train_finetune_videodiffusion.py
   ```

## ğŸ“Š Evaluation

Use `40_class_run_metrics.py` to compute:
- Top-k accuracy
- CLIP Score
- MSE / SSIM between generated and target videos

## ğŸ“Œ Notes

- This project relies on PyTorch, diffusers, Transformers, and related libraries.
- Some model definitions appear multiple times (`models.py`), which could be refactored for clarity.

